
So now we've reached the point in the lecture to describe what makes t-SNE different from its predecessor. We know that it uses a Student t-distribution to describe similarities in low dimensions versus a Gaussian but to understand why that choice was deliberately made, one needs to understand the crowding problem.

As with most concepts, the crowding problem can be best explained with a toy example. Take a look at the diagram in the top left. In the upper half is a point cloud of three points in 2 dimensions with their euclidean distances labeled. If someone came by and forced you to embed those three points in low dimensions, you'd be tasked with the problem of sacrificing some information about the relational distances of the three points.

Since we are viewing problems through the lens of t-SNE, we know that preserving the strongest similarities or shortest distances in 2 dimensions come before the weaker similarities. Hence, we need to save the two distances of 1 before trying to bully that long leg of the triangle to be root(2) in low dimensions. Thankfully, there is a configuration that preserves the shorter distances and thus matches the similarities of those pairs between low and high dimension. Not surprisingly, the longer distance became distorted. The relationship between the points forming the hypotenuse of the triangle is now off (their distance grew so that similarity went down).

This is in essence the crowding problem. While our local structure is perfect, that third point has a mismatched similarity. While we care less about larger distances being modeled as smaller distances in the embedding dimension, their contribution to the cost function is not negligible so there is a slight gradient force pushing the first and third points together, illustrated by the red arrows. This forces can cause clusters to encroach on other clusters or points to encroach on oether points, thereby squeezing points together which makes visualizing distinct clusters messy.

So, how does t-SNE fix this? Recall that the gradient now has a term in the denominator, as a result of using a student t distribution. What this does is effectively dampen this attractive force as points are more spread out in low dimensions. This extra term in the gradient gets larger quickly as points move farther apart. In our toy example, this would weaken the attractive force between the first and third points.

Said in a different way and alluding to the figure of the distributions, for points that are moderately far away in high dimensions and modeled with a moderate similarity value (see the blue line), in the embedding dimension these points can be modeled farther away without significantly affecting the cost of the embedding.